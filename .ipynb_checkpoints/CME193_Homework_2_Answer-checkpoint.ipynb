{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CME193 - Homework 2\n",
    "\n",
    "In this assignment, you'll be analyzing an interesting dataset: the passengers of the ship *Titanic*. As you are probably familiar, the *Titanic* was a ship that collided with an iceberg on April 15, 1912 and sank. About one-third of the passengers survived. We are interested in analyzing what factors are correlated with whether a person survived (whether the person was traveling with family, the person's sex, the person's age, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Loading the dataset    \n",
    "\n",
    "The dataset contains the following columns:\n",
    "- `Survived`: `1` if the person survived, `0` if not\n",
    "- `Pclass`: the ticket class the person was travelling under, `1` for 1st class, `2` for 2nd class, `3` for 3rd class\n",
    "- `Name`: name\n",
    "- `Sex`: sex, `male` for male, `female` for female\n",
    "- `Age`: age\n",
    "- `Siblings/Spouses Aboard`: the number of siblings and spouses aboard\n",
    "- `Parents/Children Aboard`: the number of parents and children aboard\n",
    "- `Fare`: the amount paid for the ticket, in pounds\n",
    "\n",
    "Eventually, we would like to use a classification algorithm to predict the `Survived` column from the other columns (besides `Name`). This means we need to convert the non-numeric columns into numeric columns (or boolean columns, for which `True` and `False` can be interpreted as `1` and `0` respectively).\n",
    "\n",
    "You can find the dataset at the following URL (from CS 109, originally from Kaggle):\n",
    "\n",
    "    http://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv\n",
    "\n",
    "Load this CSV file into a Pandas dataframe and look at the data. Convert `Pclass` and `Sex` into boolean columns; that is, create four new boolean columns, `Female`, `1st Class`, `2nd Class`, and `3rd Class`, with the appropriate values. For example, `Female` would be `True` if the person is female, `False` if the person is male.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reads the dataset into a DataFrame\n",
    "titanic = pd.read_csv('http://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv')\n",
    "titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates four new boolean columns.\n",
    "titanic['Female'] = titanic['Sex'] == 'female'\n",
    "titanic['1st Class'] = titanic['Pclass'] == 1\n",
    "titanic['2nd Class'] = titanic['Pclass'] == 2\n",
    "titanic['3rd Class'] = titanic['Pclass'] == 3\n",
    "titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Building the dataset\n",
    "Next, let's convert our dataset into NumPy arrays. Create a NumPy array `X` derived from the Pandas dataframe with the numerical and boolean columns, that is: `['Age', 'Siblings/Spouses Aboard', 'Parents/Children Aboard', 'Fare', 'Female', '1st Class', '2nd Class', '3rd Class']`. Create a NumPy vector `y` derived from the `Survived` column. Ensure that the entries of both `X` and `y` are floating point numbers. (Hint: if `a` is a NumPy array, then `a.astype(float)` converts the entries to floats.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates an array X contains 8 features.\n",
    "X = titanic[['Age', 'Siblings/Spouses Aboard', 'Parents/Children Aboard', 'Fare', \n",
    "             'Female', '1st Class', '2nd Class', '3rd Class']].to_numpy().astype(float)\n",
    "\n",
    "# Creates a vector y contains outcomes in the 'Survived' column.\n",
    "y = titanic['Survived'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Logistic regression\n",
    "Logistic regression is a classification algorithm that comes with scikit-learn. In this case, we would like to predict one of two classes, `1` if the person survived, `0` if the person did not. Normally, we would split our dataset into a training set and a test set, but for simplicity we will not do that here; instead we will train on our entire dataset.\n",
    "\n",
    "Using scikit-learn, fit a logistic regression model to the dataset we created in Question 2. What percentage of the passengers' outcomes does the model correctly predict? What does the model think about the fate of a 30-year-old male travelling alone who paid 50 pounds for his 2nd-class ticket?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fits a Logistic regression model.\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "print(model.coef_, model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the accuracy score.\n",
    "metrics.accuracy_score(model.predict(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, the percentage of the passengers' outcomes that are correctly predicted by the model is about 80.159 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicts the outcome for a new sample.\n",
    "model.predict(np.array([30, 0, 0, 50, 0, 0, 1, 0]).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the model predicts that a 30-year-old male travelling alone who paid 50 pounds for his 2nd-class ticket would not survive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Defining the model\n",
    "As an exercise, we will now do logistic regression again, but this time \"manually\" -- without the use of scikit-learn -- by solving the underlying optimization problem. To do so, we'll make use of SciPy's `special` and `optimize` packages.\n",
    "\n",
    "Consider a single observation $x$ (a single passenger), represented by a vector of length $k$. Logistic regression defines a model with two parameters, $\\alpha$ and $\\beta$, where $\\alpha$ is a number and $\\beta$ is a vector of length $k$. Assuming we know $\\alpha$ and $\\beta$, the probability that $x$ survives is the number\n",
    "$$ \\text{probability that }x\\text{ survives} = \\frac{1}{1 + \\exp(-(\\alpha + x^T \\beta)) }.$$\n",
    "Our eventual goal is to find the values for $\\alpha$ and $\\beta$ that results in a probability that best matches the observed outcome for $x$.\n",
    "\n",
    "Define a function `probability_of_surviving(alpha, beta, X)` that computes probabilities of the passengers in `X` surviving,\n",
    "$$ \\frac{1}{1 + \\exp(-(\\alpha + X \\beta )) },$$\n",
    "where:\n",
    "- `alpha` is a number $\\alpha$\n",
    "- `beta` is a vector $\\beta$ of length $k$\n",
    "- `X` is an $n$-by-$k$ matrix $X$, where each of the $n$ rows corresponds to an observation (a passenger), and each column corresponds to a feature.\n",
    "\n",
    "This function should output a vector of length $n$, with each entry being the probability that each person survives, assuming we know $\\alpha$ and $\\beta$. Note that $X\\beta$ should be interpreted as matrix multiplication, but all other operations (addition, exponential, division) operate elementwise. (Hint: check out SciPy's `special.expit`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.special as special\n",
    "\n",
    "def probability_of_surviving(alpha, beta, X):\n",
    "    '''\n",
    "    Computes probabilities of the passengers surviving.\n",
    "    \n",
    "    Args:\n",
    "        alpha (float): A number α,\n",
    "        beta (numpy.ndarray): A vector β of length k.\n",
    "        X (numpy.ndarray): An n x k matrix, with each row corresponding to \n",
    "        an observation and each column corresponding to a feature.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: A vector of length n, with each entry being \n",
    "        the probability that each person survives.\n",
    "    '''\n",
    "    power = X @ beta + alpha\n",
    "    result = np.array(special.expit(power))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Defining the loss\n",
    "Our goal is to find the $\\alpha$ and $\\beta$ values that best match the observed data $(X, y)$. To do this, we will construct a loss function that, when given $\\alpha$ and $\\beta$, characterizes how good our predictions $\\hat{y}$ are compared to the ground truth $y$. \n",
    "\n",
    "Mathematically, our loss function will be $$\n",
    "    L(\\alpha, \\beta, X, y) = \\text{sum}(\\text{KL}(y, \\hat{y})) + \\frac{1}{2}\\beta^T \\beta,\n",
    "$$ where:\n",
    "- $\\hat{y}$ is the vector of predicted probabilities (from Question 4), which is computed from $\\alpha$, $\\beta$, and $X$\n",
    "- $\\text{KL}(y, \\hat{y})$ is the *Kullback-Liebler divergence*, which measures how different the ground truth $y$ is from the predicted probabilities $\\hat{y}$\n",
    "- $\\text{sum}(\\cdot)$ sums up the entries of a vector, in this case adding up the loss for each passenger in $X$\n",
    "- $\\frac{1}{2}\\beta^T \\beta$ is a *regularization term* that prevents overfitting.\n",
    "\n",
    "Define a function `logistic_regression_loss(alpha_beta, X, y)` that computes $L(\\alpha, \\beta, X, y)$, where\n",
    "- `alpha_beta` is a vector of length $1+ k$ that contains $\\alpha$ in its first entry and $\\beta$ in the remaining entries\n",
    "- `X` is an $n$-by-$k$ matrix $X$, where each of the $n$ rows corresponds to an observation (a passenger), and each column corresponds to a feature\n",
    "- `y` is a vector $y$ of length $n$, that is 1 if the passenger survived and 0 if they did not.\n",
    "\n",
    "(Hint: check out SciPy's `special.kl_div`.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_loss(alpha_beta, X, y):\n",
    "    '''\n",
    "    Computes the values of a loss function.\n",
    "    \n",
    "    Args:\n",
    "        alpha_beta (numpy.ndarray): A vector of length k + 1, with the first entry \n",
    "        being α and the remaining entries being β.\n",
    "        X (numpy.ndarray): An n x k matrix, with each row corresponding to \n",
    "        an observation and each column corresponding to a feature.\n",
    "        y (numpy.ndarray): A vector of length n, with each entry being 1 \n",
    "        if the passenger survived and 0 otherwise.\n",
    "        \n",
    "    Returns:\n",
    "        float: The value of the loss function.\n",
    "    '''\n",
    "    alpha = alpha_beta[0]\n",
    "    beta = alpha_beta[1:]\n",
    "    kl = special.kl_div(y, probability_of_surviving(alpha, beta, X)).sum()\n",
    "    reg = (beta @ beta) / 2\n",
    "    return kl + reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: Logistic regression (the hard way)\n",
    "Use SciPy's `optimize.minimize` to find the $\\alpha$ and $\\beta$ that best explain the data $(X,y)$. In other words, find $\\alpha$ and $\\beta$ that minimizes the function `logistic_regression_loss`. Use an initial guess of $\\alpha = 0$, $\\beta = 0$. Compare your result to the $\\alpha$ and $\\beta$ computed by scikit-learn, given by `model.intercept_` and `model.coef_`. (It will not match exactly but should be somewhat close.)\n",
    "\n",
    "(Hint: you'll need to make use of the `args` argument for `optimize.minimize` to pass in `X` and `y`. Also, you may get a warning that the desired error was not achieved, but you can ignore this.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as optimize\n",
    "\n",
    "# Initial guess.\n",
    "k = np.size(X, 1)\n",
    "alpha_beta_initial = np.zeros(k + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds optimal values of α and β.\n",
    "alpha_beta = optimize.minimize(logistic_regression_loss, x0 = alpha_beta_initial, args = (X, y))\n",
    "alpha_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compares these with the result by scikit-learn.\n",
    "alpha = alpha_beta.x[0]\n",
    "beta = alpha_beta.x[1:]\n",
    "\n",
    "print(\"Result by SciPy:\")\n",
    "print(f\"alpha = {alpha}\")\n",
    "print(f\"beta = {beta}\")\n",
    "print()\n",
    "\n",
    "print(\"Result by scikit-learn:\")\n",
    "print(f\"alpha = {model.intercept_[0]}\")\n",
    "print(f\"beta = {model.coef_[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7: Predictions\n",
    "\n",
    "With what probability does the model learned in Question 6 think a 30-year-old male travelling alone who paid 50 pounds for his 2nd-class ticket will survive? What percentage of the passengers' outcomes does the model correctly predict, if we say that the model predicts survival if the probability of survival is greater than 0.5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicts the outcome for a new sample.\n",
    "passenger = np.array([[30, 0, 0, 50, 0, 0, 1, 0]])\n",
    "print(\"Probability of Surviving: {}\"\n",
    "      .format(probability_of_surviving(alpha, beta, passenger)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the accuracy score.\n",
    "outcome = (probability_of_surviving(alpha, beta, X) > 0.5).astype(float)\n",
    "metrics.accuracy_score(outcome, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, the percentage of the passengers' outcomes that are correctly predicted by the model is about 80.609 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission instructions\n",
    "\n",
    "Save this notebook (`CME193_Homework_2.ipynb`), and submit it on Canvas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
